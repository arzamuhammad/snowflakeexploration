{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# # Set warehouse if needed\n",
    "# session.sql(\"USE WAREHOUSE YOUR_WAREHOUSE_NAME\").collect()\n",
    "# session.sql(\"USE DATABASE YOUR_DATABASE_NAME\").collect() \n",
    "# session.sql(\"USE SCHEMA YOUR_SCHEMA_NAME\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "def get_last_partition(session, table_name):\n",
    "    \"\"\"\n",
    "    Get the last partition from a table in Snowflake\n",
    "    Note: Snowflake doesn't use Hive-style partitions, so this might need adjustment\n",
    "    based on your table structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If your table has a partition column, adjust this query accordingly\n",
    "        result = session.sql(f\"\"\"\n",
    "            SELECT MAX(partition_column) as last_partition \n",
    "            FROM {table_name}\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if result:\n",
    "            return result[0]['LAST_PARTITION']\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting last partition: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(session, env):\n",
    "    \"\"\"\n",
    "    Process data in Snowflake equivalent to the original PySpark job\n",
    "    Based on merge_revenue_ifrs_dd_accrual configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define table - Snowflake uses database.schema.table format\n",
    "    table_1 = f'{env[\"table_1\"][\"database\"]}.{env[\"table_1\"][\"schema\"]}.{env[\"table_1\"][\"table\"]}'\n",
    "    \n",
    "    # Define periode (matching the catalog filter patterns)\n",
    "    event_date = env[\"table_1\"][\"filter_d2\"]  # day=2 (2 days ago)\n",
    "    load_date = env[\"table_1\"][\"filter_d0\"]   # day=0 (today)\n",
    "    \n",
    "    print(f\"Running for event_date={event_date} and load_date={load_date}\")\n",
    "    \n",
    "    # Snowflake SQL query - converted from the original PySpark SQL\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        DATEADD(day, -1, trx_date) AS trx_date,\n",
    "        -- Simplified purchase_date calculation (hex conversion may need adjustment)\n",
    "        CASE \n",
    "            WHEN SUBSTR(transaction_id, 3, 3) = '000' \n",
    "            THEN SUBSTR(transaction_id, 1, 10)  -- Temporary simplified version\n",
    "            ELSE SUBSTR(transaction_id, 1, 10)  -- Temporary simplified version\n",
    "        END AS purchase_date,\n",
    "        transaction_id,\n",
    "        subscriber_id AS subs_id,\n",
    "        msisdn,\n",
    "        CAST(price_plan_id AS INTEGER) AS price_plan_id,\n",
    "        brand,\n",
    "        2 AS pre_post_flag,\n",
    "        cust_type_desc,\n",
    "        cust_subtype_desc,\n",
    "        customer_group AS customer_sub_segment,\n",
    "        lac,\n",
    "        ci,\n",
    "        lacci_id,\n",
    "        node,\n",
    "        CASE \n",
    "            WHEN area_sales IS NULL OR area_sales = '' THEN 'UNKNOWN' \n",
    "            ELSE area_sales \n",
    "        END AS area_sales,\n",
    "        CASE \n",
    "            WHEN region_sales IS NULL OR region_sales = '' THEN 'UNKNOWN' \n",
    "            ELSE region_sales \n",
    "        END AS region_sales,\n",
    "        CASE \n",
    "            WHEN branch IS NULL OR branch = '' THEN 'UNKNOWN' \n",
    "            ELSE branch \n",
    "        END AS branch,\n",
    "        CASE \n",
    "            WHEN subbranch IS NULL OR subbranch = '' THEN 'UNKNOWN' \n",
    "            ELSE subbranch \n",
    "        END AS subbranch,\n",
    "        CASE \n",
    "            WHEN cluster_sales IS NULL OR cluster_sales = '' THEN 'UNKNOWN' \n",
    "            ELSE cluster_sales \n",
    "        END AS cluster_sales,\n",
    "        CASE \n",
    "            WHEN provinsi IS NULL OR provinsi = '' THEN 'UNKNOWN' \n",
    "            ELSE provinsi \n",
    "        END AS provinsi,\n",
    "        CASE \n",
    "            WHEN kabupaten IS NULL OR kabupaten = '' THEN 'UNKNOWN' \n",
    "            ELSE kabupaten \n",
    "        END AS kabupaten,\n",
    "        CASE \n",
    "            WHEN kecamatan IS NULL OR kecamatan = '' THEN 'UNKNOWN' \n",
    "            ELSE kecamatan \n",
    "        END AS kecamatan,\n",
    "        CASE \n",
    "            WHEN kelurahan IS NULL OR kelurahan = '' THEN 'UNKNOWN' \n",
    "            ELSE kelurahan \n",
    "        END AS kelurahan,\n",
    "        CAST(lacci_closing_flag AS INTEGER) AS lacci_closing_flag,\n",
    "        sigma_business_id,\n",
    "        sigma_rules_id,\n",
    "        SUBSTR(transaction_id, 19, 13) AS sku,\n",
    "        l1_payu,\n",
    "        l2_service_type,\n",
    "        l3_allowance_type,\n",
    "        l4_product_category,\n",
    "        l5_product,\n",
    "        '' AS l1_ias,\n",
    "        '' AS l2_ias,\n",
    "        '' AS l3_ias,\n",
    "        commercial_name,\n",
    "        channel,\n",
    "        validity AS pack_validity,\n",
    "        CAST(SUM(rev) AS DECIMAL(38,15)) AS rev_per_usage,\n",
    "        CAST(SUM(0) AS DECIMAL(38,15)) AS rev_seized,\n",
    "        CAST(SUM(0) AS INTEGER) AS dur,\n",
    "        CAST(COUNT(DISTINCT transaction_id) AS INTEGER) AS trx,\n",
    "        CAST(SUM(0) AS BIGINT) AS vol,\n",
    "        CAST(customer_id AS INTEGER) AS cust_id,\n",
    "        charge_code AS profile_name,\n",
    "        amdd_charge_code AS quota_name,\n",
    "        proration_ind AS service_filter,\n",
    "        offer_name AS price_plan_name,\n",
    "        channel_id,\n",
    "        '' AS site_id,\n",
    "        '' AS site_name,\n",
    "        region_hlr,\n",
    "        city_hlr,\n",
    "        '{load_date}' AS load_date,\n",
    "        DATEADD(day, -1, '{event_date}') AS event_date,\n",
    "        'ACCRUAL' AS SOURCE\n",
    "    FROM {table_1}\n",
    "    WHERE event_date = DATEADD(day, -1, '{load_date}')\n",
    "      AND amdd_charge_code = 'SFEE'\n",
    "    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,45,46,47,48,49,50,51,52,53,54,55,56,57\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    df = session.sql(sql_query)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de5e9e-7bd8-4ffd-98a7-1afae698f125",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    env = {\n",
    "        \"table_1\": {\n",
    "            \"database\": \"TELKOMSEL_POC\",        # Your database name\n",
    "            \"schema\": \"RAW\",     # Your schema name  \n",
    "            \"table\": \"IFRS_ACCRUAL_PRODUCT_DAILY_POC_TOKENIZED\",       # Your table name\n",
    "            \"filter_d2\": \"2025-04-01\",         # Your event date\n",
    "            \"filter_d0\": \"2025-04-02\"          # Your load date\n",
    "        }\n",
    "    }\n",
    "    # Process the data\n",
    "    result_df = process_data(session, env)\n",
    "    \n",
    "    # Show results\n",
    "    print(\"Processing completed successfully!\")\n",
    "    result_df.show(10)  # Show first 10 rows\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "644358014528",
   "authorName": "DELOITTE2",
   "lastEditTime": 1752470453013,
   "notebookId": "2e6kru6devqzrl2lp7nc",
   "sessionId": "aeb664bb-b94b-4b5a-aae2-8247ecb73c8f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
