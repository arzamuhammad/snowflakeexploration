{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2e6kru6devqzrl2lp7nc",
   "authorId": "644358014528",
   "authorName": "DELOITTE2",
   "authorEmail": "",
   "sessionId": "aeb664bb-b94b-4b5a-aae2-8247ecb73c8f",
   "lastEditTime": 1752480970777
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "import snowflake.snowpark as snowpark\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import *\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# # Set warehouse if needed\n# session.sql(\"USE WAREHOUSE YOUR_WAREHOUSE_NAME\").collect()\n# session.sql(\"USE DATABASE YOUR_DATABASE_NAME\").collect() \n# session.sql(\"USE SCHEMA YOUR_SCHEMA_NAME\").collect()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "def get_last_partition(session, table_name):\n    \"\"\"\n    Get the last partition from a table in Snowflake\n    Note: Snowflake doesn't use Hive-style partitions, so this might need adjustment\n    based on your table structure\n    \"\"\"\n    try:\n        # If your table has a partition column, adjust this query accordingly\n        result = session.sql(f\"\"\"\n            SELECT MAX(partition_column) as last_partition \n            FROM {table_name}\n        \"\"\").collect()\n        \n        if result:\n            return result[0]['LAST_PARTITION']\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error getting last partition: {e}\")\n        return None\n\ndef process_data(session, env):\n    \"\"\"\n    Process data in Snowflake equivalent to the original PySpark job\n    Based on merge_revenue_ifrs_dd_overusage configuration\n    \"\"\"\n    \n    # Define table - Snowflake uses database.schema.table format\n    table_1 = f'{env[\"table_1\"][\"database\"]}.{env[\"table_1\"][\"schema\"]}.{env[\"table_1\"][\"table\"]}'\n    \n    # Define periode (matching the catalog filter patterns)\n    event_date = env[\"table_1\"][\"filter_d2\"]  # day=2 (2 days ago)\n    load_date = env[\"table_1\"][\"filter_d0\"]   # day=0 (today)\n    \n    print(f\"Running for event_date={event_date} and load_date={load_date}\")\n    \n    # Snowflake SQL query - converted from the original PySpark SQL\n    sql_query = f\"\"\"\n    SELECT \n        '{event_date}' AS trx_date,\n        purchase_date_2 AS purchase_date,\n        transaction_id,\n        '' AS subs_id,\n        msisdn,\n        reserve1::INTEGER AS price_plan_id,\n        brand,\n        1 AS pre_post_flag,\n        cust_type AS cust_type_desc,\n        cust_subtype AS cust_subtype_desc,\n        '' AS customer_sub_segment,\n        '' AS lac,\n        '' AS ci,\n        '' AS lacci_id,\n        node_type AS node,\n        'UNKNOWN' AS area_sales,\n        CASE \n            WHEN region IS NULL OR region = '' THEN 'UNKNOWN' \n            ELSE region \n        END AS region_sales,\n        CASE \n            WHEN branch IS NULL OR branch = '' THEN 'UNKNOWN' \n            ELSE branch \n        END AS branch,\n        'UNKNOWN' AS subbranch,\n        CASE \n            WHEN cluster IS NULL OR cluster = '' THEN 'UNKNOWN' \n            ELSE cluster \n        END AS cluster_sales,\n        'UNKNOWN' AS provinsi,\n        'UNKNOWN' AS kabupaten,\n        'UNKNOWN' AS kecamatan,\n        'UNKNOWN' AS kelurahan,\n        NULL AS lacci_closing_flag,\n        bid AS sigma_business_id,\n        '' AS sigma_rules_id,\n        '' AS sku,\n        '' AS l1_payu,\n        '' AS l2_service_type,\n        '' AS l3_allowance_type,\n        '' AS l4_product_category,\n        '' AS l5_product,\n        l1_ias,\n        l2_ias,\n        l3_ias,\n        '' AS commercial_name,\n        '' AS channel,\n        '' AS pack_validity,\n        SUM(pi_value_final)::DECIMAL(38,15) AS rev_per_usage,\n        SUM(0)::DECIMAL(38,15) AS rev_seized,\n        SUM(0)::INTEGER AS dur,\n        SUM(0)::INTEGER AS trx,\n        SUM(0)::BIGINT AS vol,\n        NULL AS cust_id,\n        '' AS profile_name,\n        '' AS quota_name,\n        '' AS service_filter,\n        '' AS price_plan_name,\n        SUBSTR(transaction_id, 1, 2) AS channel_id,\n        '' AS site_id,\n        '' AS site_name,\n        '' AS region_hlr,\n        '' AS city_hlr,\n        '{load_date}' AS load_date,\n        '{event_date}' AS event_date,\n        'OVERUSAGE' AS SOURCE\n    FROM {table_1}\n    WHERE load_date = '{event_date}'\n      AND expiry_month > '{event_date}'\n      AND flag = 'alloc_<_rev'\n    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,45,46,47,48,49,50,51,52,53,54,55,56,57\n    \"\"\"\n    \n    # Execute the query\n    df = session.sql(sql_query)\n    \n    return df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c0de5e9e-7bd8-4ffd-98a7-1afae698f125",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "try:\n    env = {\n        \"table_1\": {\n            \"database\": \"TELKOMSEL_POC\",                                        # From catalog\n            \"schema\": \"RAW\",                                           # Update with your schema\n            \"table\": \"IFRS_CONS_RATE_AND_OUTSTANDING_DD_POC_TOKENIZED\",   # From catalog\n            \"filter_d2\": \"2025-04-01\",     # Hardcoded: 1st April 2025 (event_date)\n            \"filter_d0\": \"2025-04-01\"      # Hardcoded: 2nd April 2025 (load_date)\n        }\n    }\n    \n    # Process the data\n    result_df = process_data(session, env)\n    \n    # Show results\n    print(\"Processing completed successfully!\")\n    result_df.show(10)  # Show first 10 rows\n    \nexcept Exception as e:\n    print(f\"Error processing data: {e}\")\n    raise",
   "execution_count": null
  }
 ]
}