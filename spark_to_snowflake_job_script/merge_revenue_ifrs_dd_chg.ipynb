{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2e6kru6devqzrl2lp7nc",
   "authorId": "644358014528",
   "authorName": "DELOITTE2",
   "authorEmail": "",
   "sessionId": "aeb664bb-b94b-4b5a-aae2-8247ecb73c8f",
   "lastEditTime": 1752478244816
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "import snowflake.snowpark as snowpark\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import *\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# # Set warehouse if needed\n# session.sql(\"USE WAREHOUSE YOUR_WAREHOUSE_NAME\").collect()\n# session.sql(\"USE DATABASE YOUR_DATABASE_NAME\").collect() \n# session.sql(\"USE SCHEMA YOUR_SCHEMA_NAME\").collect()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "def get_last_partition(session, table_name):\n    \"\"\"\n    Get the last partition from a table in Snowflake\n    Note: Snowflake doesn't use Hive-style partitions, so this might need adjustment\n    based on your table structure\n    \"\"\"\n    try:\n        # If your table has a partition column, adjust this query accordingly\n        result = session.sql(f\"\"\"\n            SELECT MAX(partition_column) as last_partition \n            FROM {table_name}\n        \"\"\").collect()\n        \n        if result:\n            return result[0]['LAST_PARTITION']\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error getting last partition: {e}\")\n        return None\n\ndef process_data(session, env):\n    \"\"\"\n    Process data in Snowflake equivalent to the original PySpark CHG job\n    Based on merge_revenue_ifrs_dd_chg configuration\n    \"\"\"\n    \n    # Define table - Snowflake uses database.schema.table format\n    table_1 = f'{env[\"table_1\"][\"database\"]}.{env[\"table_1\"][\"schema\"]}.{env[\"table_1\"][\"table\"]}'\n    \n    # Define periode (matching the catalog filter patterns)\n    event_date = env[\"table_1\"][\"filter_d2\"]  # day=2 (2 days ago)\n    load_date = env[\"table_1\"][\"filter_d0\"]   # day=0 (today)\n    \n    print(f\"Running for event_date={event_date} and load_date={load_date}\")\n    \n    # Snowflake SQL query - converted from the original PySpark SQL\n    sql_query = f\"\"\"\n    SELECT \n        trx_date,\n        CONCAT(SUBSTR(timestamp_ifrs, 1, 4), '-', SUBSTR(timestamp_ifrs, 5, 2), '-', SUBSTR(timestamp_ifrs, 7, 2)) AS purchase_date,\n        transaction_id,\n        subs_id,\n        msisdn,\n        price_plan_id,\n        brand,\n        pre_post_flag,\n        cust_type_desc,\n        cust_subtype_desc,\n        customer_sub_segment,\n        lac,\n        ci,\n        lacci_id_ifrs AS lacci_id,\n        node,\n        CASE \n            WHEN area_sales IS NULL OR area_sales = '' THEN 'UNKNOWN' \n            ELSE area_sales \n        END AS area_sales,\n        CASE \n            WHEN region_sales IS NULL OR region_sales = '' THEN 'UNKNOWN' \n            ELSE region_sales \n        END AS region_sales,\n        CASE \n            WHEN branch IS NULL OR branch = '' THEN 'UNKNOWN' \n            ELSE branch \n        END AS branch,\n        CASE \n            WHEN subbranch IS NULL OR subbranch = '' THEN 'UNKNOWN' \n            ELSE subbranch \n        END AS subbranch,\n        CASE \n            WHEN cluster_sales IS NULL OR cluster_sales = '' THEN 'UNKNOWN' \n            ELSE cluster_sales \n        END AS cluster_sales,\n        CASE \n            WHEN provinsi IS NULL OR provinsi = '' THEN 'UNKNOWN' \n            ELSE provinsi \n        END AS provinsi,\n        CASE \n            WHEN kabupaten IS NULL OR kabupaten = '' THEN 'UNKNOWN' \n            ELSE kabupaten \n        END AS kabupaten,\n        CASE \n            WHEN kecamatan IS NULL OR kecamatan = '' THEN 'UNKNOWN' \n            ELSE kecamatan \n        END AS kecamatan,\n        CASE \n            WHEN kelurahan IS NULL OR kelurahan = '' THEN 'UNKNOWN' \n            ELSE kelurahan \n        END AS kelurahan,\n        lacci_closing_flag,\n        sigma_business_id,\n        sigma_rules_id,\n        sku,\n        l1_payu,\n        l2_service_type,\n        l3_allowance_type,\n        l4_product_category,\n        l5_product,\n        '' AS l1_ias,\n        '' AS l2_ias,\n        '' AS l3_ias,\n        commercial_name,\n        channel,\n        validity AS pack_validity,\n        SUM(rev_per_usage)::DECIMAL(38,15) AS rev_per_usage,\n        SUM(0)::DECIMAL(38,15) AS rev_seized,\n        SUM(call_duration)::INTEGER AS dur,\n        SUM(event_allowance_consumed)::INTEGER AS trx,\n        SUM(total_volume)::BIGINT AS vol,\n        cust_id,\n        '' AS profile_name,\n        item_id AS quota_name,\n        service_filter,\n        price_plan_name,\n        channel_id,\n        site_id,\n        site_name,\n        region_hlr,\n        city_hlr,\n        '{load_date}' AS load_date,\n        event_date,\n        'CHG' AS SOURCE\n    FROM {table_1}\n    WHERE event_date = '{event_date}'\n      AND pre_post_flag = '1'\n    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,45,46,47,48,49,50,51,52,53,54,55,56,57\n    \"\"\"\n    \n    # Execute the query\n    df = session.sql(sql_query)\n    \n    return df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c0de5e9e-7bd8-4ffd-98a7-1afae698f125",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "try:\n    env = {\n        \"table_1\": {\n            \"database\": \"TELKOMSEL_POC\",                      # From catalog\n            \"schema\": \"RAW\",                         # Update with your schema\n            \"table\": \"IFRS_TC_CHG_POC_TOKENIZED\",      # From catalog\n            \"filter_d2\": \"2025-04-02\",                 # Hardcoded: 1st April 2025 (event_date)\n            \"filter_d0\": \"tc_chg_poc\"                  # Hardcoded: 2nd April 2025 (load_date)\n        }\n    }\n    \n    # Process the data\n    result_df = process_data(session, env)\n    \n    # Show results\n    print(\"Processing completed successfully!\")\n    result_df.show(10)  # Show first 10 rows\n    \nexcept Exception as e:\n    print(f\"Error processing data: {e}\")\n    raise",
   "execution_count": null
  }
 ]
}