Element,ProjectId,FileId,Count,Alias,Kind,Line,PackageName,Supported,Automated,Status,Statement,SessionId,SnowConvertCoreVersion,SnowparkVersion,CellId,ExecutionId,ParametersInfo,Technology
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,f,Module,5,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,import pyspark.sql.functions as f,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,SparkSession,Class,6,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,from pyspark.sql import SparkSession,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,col,Function,7,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,lag,Function,7,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lead,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,lead,Function,7,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,Window,Class,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DateType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,DateType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DoubleType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,DoubleType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.IntegerType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,IntegerType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StringType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,StringType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructField,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,StructField,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,StructType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.TimestampType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,TimestampType,Class,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,pyspark,Module,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,import pyspark,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,SparkContext,Class,14,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,from pyspark import SparkContext,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,SparkConf,Class,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,from pyspark.conf import SparkConf,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,DataFrame,Class,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,SparkSession,Class,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,F,Module,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,from pyspark.sql import functions as F,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,T,Module,18,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,from pyspark.sql import types as T,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,Window,Class,19,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.version.__version__,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Variable,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotDefined,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf.setMaster,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,9,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.setAppName,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,12,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,13,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,14,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,18,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,22,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-rerun"").set(""spark.dynamicAllocation.maxExecutors"", ""50"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.context.SparkContext,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext.getOrCreate,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""conf"",""ParamType"":""SparkConf""}]",Python
pyspark.sql.session.SparkSession,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,SparkSession(sc),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.context.SparkContext"",""ParamType"":""SparkContext""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"F.substring('calling_date', 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"F.substring('calling_date', 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('imsi').cast(StringType()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('imsi').cast(StringType()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('imsi').cast(StringType()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.types.StringType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.cast,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.lit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,10,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(8),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(8),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,21,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,21,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,22,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,22,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,23,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,23,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.pow,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,23,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sin,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,23,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.cos,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.cos,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.pow,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.radians,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.radians,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.sin,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.atan2,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,26,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.round,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,26,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""float"",""ParamType"":""float""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,Window.partitionBy('msisdn').orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,Window.partitionBy('msisdn').orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,Window.partitionBy('msisdn').orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(f.col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(f.col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(f.col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(f.col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.date_format(col('datetime'), 'E')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.date_format,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.date_format(col('datetime'), 'E')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('event_day') == 'Sat' ) | ( col('event_day') == 'Sun' ), 'weekend').otherwise('weekday')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('event_day') == 'Sat' ) | ( col('event_day') == 'Sun' ), 'weekend').otherwise('weekday')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('event_day'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('event_day') == 'Sat' ) | ( col('event_day') == 'Sun' ), 'weekend').otherwise('weekday')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,44,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('day_type'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,45,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('latitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,45,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('latitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,45,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('latitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,46,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('longitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,46,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('longitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,46,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('longitude').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,47,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('datetime').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,47,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('datetime').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,47,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lag('datetime').over(part),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,48,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.unix_timestamp('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,48,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.unix_timestamp('prev_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,48,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,f.unix_timestamp('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,48,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,f.unix_timestamp('prev_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,49,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,49,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,49,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('prev_latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,49,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('prev_longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,50,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(col('distance') / ( col('time_seconds') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,50,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('distance'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,50,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('time_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,50,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(col('distance') / ( col('time_seconds') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,51,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('speed'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.format_string(""%04d-M%02d"", f.year(col('datetime')), f.month(col('datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.month(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.year(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.format_string,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,False,False,NotSupported,"f.format_string(""%04d-M%02d"", f.year(col('datetime')), f.month(col('datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.month,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.month(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.year,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,52,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.year(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,53,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.to_date(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,53,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,53,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.to_date(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,54,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.hour(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,54,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.hour,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,54,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.hour(col('datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,55,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('hour') >= home_start ) & ( col('hour') <= home_end ), 'home')\ .when(( col('hour') >= work_start ) & ( col('hour') <= work_end ), 'work')\ .when(( col('hour') >= home_start2 ) & ( col('hour') <= home_end2 ), 'home')\ .otherwise('others')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,55,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,55,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('hour') >= home_start ) & ( col('hour') <= home_end ), 'home')\ .when(( col('hour') >= work_start ) & ( col('hour') <= work_end ), 'work')\ .when(( col('hour') >= home_start2 ) & ( col('hour') <= home_end2 ), 'home')\ .otherwise('others')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,56,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('hour') >= home_start ) & ( col('hour') <= home_end ), 'home')\ .when(( col('hour') >= work_start ) & ( col('hour') <= work_end ), 'work')\ .when(( col('hour') >= home_start2 ) & ( col('hour') <= home_end2 ), 'home')\ .otherwise('others')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,56,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,57,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('hour') >= home_start ) & ( col('hour') <= home_end ), 'home')\ .when(( col('hour') >= work_start ) & ( col('hour') <= work_end ), 'work')\ .when(( col('hour') >= home_start2 ) & ( col('hour') <= home_end2 ), 'home')\ .otherwise('others')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,57,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,58,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(( col('hour') >= home_start ) & ( col('hour') <= home_end ), 'home')\ .when(( col('hour') >= work_start ) & ( col('hour') <= work_end ), 'work')\ .when(( col('hour') >= home_start2 ) & ( col('hour') <= home_end2 ), 'home')\ .otherwise('others')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,59,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,59,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,60,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,60,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,62,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy(['msisdn', 'month', 'h3_8', 'activity_kec', 'hour_category'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,62,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy(['msisdn', 'month', 'h3_8', 'activity_kec', 'hour_category'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,64,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy(['msisdn', 'month', 'hour_category'])\ .orderBy([col('total_date').desc(), col('total_count').desc(), 'entropy'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,64,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy(['msisdn', 'month', 'hour_category'])\ .orderBy([col('total_date').desc(), col('total_count').desc(), 'entropy'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,65,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('total_count').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,65,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('total_date').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,65,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('total_count').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,65,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('total_date').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,65,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy(['msisdn', 'month', 'hour_category'])\ .orderBy([col('total_date').desc(), col('total_count').desc(), 'entropy'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[any]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,70,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.count('*').alias('N'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,70,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.count('*').alias('N'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,70,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.count('*').alias('N'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,71,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('date').alias('N_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,71,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('date').alias('N_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,71,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('date').alias('N_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,73,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('N_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,74,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N_date').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,74,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N_date').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,74,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N_date').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,75,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,75,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,75,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('N').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,76,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('N'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,76,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('total_count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,76,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('N'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,76,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('total_count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum(f.col('prob') * f.log2(f.col('prob'))).over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Module,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('prob'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.log2(f.col('prob')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(- f.sum(f.col('prob') * f.log2(f.col('prob'))).over(part2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum(f.col('prob') * f.log2(f.col('prob'))).over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,2,,Function,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.col('prob'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.log2,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.log2(f.col('prob')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.round,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.round(- f.sum(f.col('prob') * f.log2(f.col('prob'))).over(part2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sum,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,77,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum(f.col('prob') * f.log2(f.col('prob'))).over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,78,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.dense_rank().over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,78,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.dense_rank().over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.dense_rank,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,78,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.dense_rank().over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,79,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,79,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('rank'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('h3_8'))).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('h3_8'))).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('h3_8'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('h3_8'))).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,86,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('h3_8'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('activity_kec'))).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('activity_kec'))).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('activity_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('activity_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('activity_kec'))).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,87,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('activity_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('entropy'))).alias('home_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('entropy'))).alias('home_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('entropy'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'home', col('entropy'))).alias('home_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,88,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'home', col('entropy'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('h3_8'))).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('h3_8'))).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('h3_8'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('h3_8'))).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,89,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('h3_8'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('activity_kec'))).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('activity_kec'))).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('activity_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('activity_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('activity_kec'))).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,90,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('activity_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('entropy'))).alias('work_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('entropy'))).alias('work_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('entropy'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('hour_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.min,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.min(f.when(col('hour_category') == 'work', col('entropy'))).alias('work_entropy')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,91,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.when(col('hour_category') == 'work', col('entropy'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.substr,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,93,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('home_kec').substr(0, 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,93,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('home_kec').substr(0, 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.substr,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,94,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('work_kec').substr(0, 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,94,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('work_kec').substr(0, 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.substr,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('month').substr(0, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.substr,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('month').substr(7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.concat_ws('-', col('month').substr(0, 4), col('month').substr(7, 2), f.lit('01'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit('01'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.to_date(f.concat_ws('-', col('month').substr(0, 4), col('month').substr(7, 2), f.lit('01')), 'yyyy-MM-dd')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('month').substr(0, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('month').substr(7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.concat_ws,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,"f.concat_ws('-', col('month').substr(0, 4), col('month').substr(7, 2), f.lit('01'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.lit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit('01'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,95,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.to_date(f.concat_ws('-', col('month').substr(0, 4), col('month').substr(7, 2), f.lit('01')), 'yyyy-MM-dd')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,96,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.add_months(col('date'), - 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.add_months,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,96,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.add_months(col('date'), - 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,96,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,97,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.last_day(col('date')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,97,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.last_day,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,97,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.last_day(col('date')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.udf(lambda h3_str, size:h3.cell_to_parent(h3_str, size) if h3_str else None, returnType = StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,"f.udf(lambda h3_str, size:h3.cell_to_parent(h3_str, size) if h3_str else None, returnType = StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.AST.PyExprLambDef"",""ParamType"":""Snowflake.SnowConvert.Python.AST.PyExprLambDef""},{""ParamName"":""returnType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"F.udf(lambda lat, lon, size:h3.latlng_to_cell(lat, lon, size) if lat != None and lon != None else None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,"F.udf(lambda lat, lon, size:h3.latlng_to_cell(lat, lon, size) if lat != None and lon != None else None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.AST.PyExprLambDef"",""ParamType"":""Snowflake.SnowConvert.Python.AST.PyExprLambDef""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pe_bps.indonesia_h3_38prov'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pe_bps.indonesia_h3_38prov'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,6,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pe_bps.wisnus_sample5_lbs_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,6,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pe_bps.wisnus_sample5_lbs_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,ue_monthly.repartition(100).write.partitionBy('month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,16,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.write,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_monthly.repartition(100).write.partitionBy('month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,16,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameWriter.mode,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_monthly.repartition(100).write.partitionBy('month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,16,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,ue_monthly.repartition(100).write.partitionBy('month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,16,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.saveAsTable,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,ue_monthly.repartition(100).write.partitionBy('month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,16,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.sql(q).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,17,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.sql,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.sql(q).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,17,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.limit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample').limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample').limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample').limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample').limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.asc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_entropy').asc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_N_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('latest_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_entropy').asc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_N_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('latest_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('home_N_month').desc(), col('home_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('home_N_month').desc(), col('home_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('home_N_month').desc(), col('home_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[pyspark.sql.column.Column]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.asc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_entropy').asc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('latest_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.desc,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_N_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('latest_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_entropy').asc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_N_month').desc(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Class,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('work_N_month').desc(), col('work_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('work_N_month').desc(), col('work_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"Window.partitionBy('msisdn').orderBy([col('work_N_month').desc(), col('work_entropy').asc(), col('latest_month').desc()])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[pyspark.sql.column.Column]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.between,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('date').between(start_date, end_date)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_monthly\ .filter(col('date').between(start_date, end_date))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,8,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"col('date').between(start_date, end_date)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,11,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,12,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,13,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('home_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,13,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('home_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,13,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('home_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,14,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('home_entropy').alias('home_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,14,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('home_entropy').alias('home_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,14,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('home_entropy').alias('home_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,15,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_home),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_home),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.row_number,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,17,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_home),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,18,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,18,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('rn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,19,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,19,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(period_month[:4] + '-M' + period_month[- 2:]),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,19,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(period_month[:4] + '-M' + period_month[- 2:]),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,20,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'home_h3_8', 'home_kab', 'home_kec')\ .agg(f.countDistinct('month').alias('home_N_month'), f.sum('home_entropy').alias('home_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_home))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,23,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,24,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('work_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('work_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,25,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.countDistinct('month').alias('work_N_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,26,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('work_entropy').alias('work_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,26,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('work_entropy').alias('work_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,26,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.sum('work_entropy').alias('work_entropy'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,27,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,27,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,27,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.max('month').alias('latest_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_work),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_work),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.row_number,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,29,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.row_number().over(part_work),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,30,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,30,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('rn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,31,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,31,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(period_month[:4] + '-M' + period_month[- 2:]),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,31,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,f.lit(period_month[:4] + '-M' + period_month[- 2:]),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,32,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly\ .groupBy('msisdn', 'work_h3_8', 'work_kab', 'work_kec')\ .agg(f.countDistinct('month').alias('work_N_month'), f.sum('work_entropy').alias('work_entropy'), f.max('month').alias('latest_month'))\ .withColumn('rn', f.row_number().over(part_work))\ .filter(col('rn') == 1)\ .withColumn('event_month', f.lit(period_month[:4] + '-M' + period_month[- 2:]))\ .drop('rn', 'latest_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,35,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"home_semesterly\ .join(work_semesterly, ['msisdn', 'event_month'], 'full')\ .select('event_month', 'msisdn', f.coalesce(col('home_h3_8'), col('work_h3_8')).alias('home_h3_8'), f.coalesce(col('work_h3_8'), col('home_h3_8')).alias('work_h3_8'), f.coalesce(col('home_kab'), col('work_kab')).alias('home_kab'), f.coalesce(col('home_kec'), col('work_kec')).alias('home_kec'), f.coalesce(col('work_kab'), col('home_kab')).alias('work_kab'), f.coalesce(col('work_kec'), col('home_kec')).alias('work_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,36,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"home_semesterly\ .join(work_semesterly, ['msisdn', 'event_month'], 'full')\ .select('event_month', 'msisdn', f.coalesce(col('home_h3_8'), col('work_h3_8')).alias('home_h3_8'), f.coalesce(col('work_h3_8'), col('home_h3_8')).alias('work_h3_8'), f.coalesce(col('home_kab'), col('work_kab')).alias('home_kab'), f.coalesce(col('home_kec'), col('work_kec')).alias('home_kec'), f.coalesce(col('work_kab'), col('home_kab')).alias('work_kab'), f.coalesce(col('work_kec'), col('home_kec')).alias('work_kec'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,38,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_h3_8'), col('work_h3_8')).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,38,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_h3_8'), col('work_h3_8')).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,38,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_h3_8'), col('work_h3_8')).alias('home_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,38,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,38,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_h3_8'), col('home_h3_8')).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_h3_8'), col('home_h3_8')).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_h3_8'), col('home_h3_8')).alias('work_h3_8')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,39,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kab'), col('work_kab')).alias('home_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kab'), col('work_kab')).alias('home_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kab'), col('work_kab')).alias('home_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,40,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,41,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kec'), col('work_kec')).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,41,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kec'), col('work_kec')).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,41,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('home_kec'), col('work_kec')).alias('home_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,41,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,41,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kab'), col('home_kab')).alias('work_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kab'), col('home_kab')).alias('work_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kab'), col('home_kab')).alias('work_kab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,42,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kec'), col('home_kec')).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kec'), col('home_kec')).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.coalesce,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"f.coalesce(col('work_kec'), col('home_kec')).alias('work_kec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('home_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,43,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,col('work_kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.printSchema,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_semesterly.printSchema(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,ue_semesterly.repartition(100).write.partitionBy('event_month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,27,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.write,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_semesterly.repartition(100).write.partitionBy('event_month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,27,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameWriter.mode,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_semesterly.repartition(100).write.partitionBy('event_month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,27,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.partitionBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Rename,ue_semesterly.repartition(100).write.partitionBy('event_month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,27,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.readwriter.DataFrameWriter.saveAsTable,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,16,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Transformation,ue_semesterly.repartition(100).write.partitionBy('event_month').mode('append').saveAsTable('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,27,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,30,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_monthly_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,30,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.limit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_monthly.limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,31,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_monthly.limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,31,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.sql(q).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,32,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.sql,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.sql(q).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,32,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_monthly.groupBy('month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_monthly.groupBy('month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_monthly.groupBy('month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,33,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,35,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,35,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.limit,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_semesterly.limit(12).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,36,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,ue_semesterly.limit(12).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,36,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly.groupBy('event_month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('event_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,1,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly.groupBy('event_month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('event_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,2,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Module,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,3,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_1. Pipeline Periodically - Usual Environment with Entropy.ipynb,1,,Function,4,notebook.Sample_1. Pipeline Periodically - Usual Environment with Entropy,True,True,Direct,"ue_semesterly.groupBy('event_month').agg(F.count('msisdn').alias('num_row'), F.countDistinct('msisdn').alias('num_msisdn')).toPandas().sort_values('event_month')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,38,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,f,Module,5,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,import pyspark.sql.functions as f,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,SparkSession,Class,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,from pyspark.sql import SparkSession,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,col,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,lag,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lead,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,lead,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,Window,Class,8,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DateType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,DateType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DoubleType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,DoubleType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.IntegerType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,IntegerType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StringType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,StringType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructField,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,StructField,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,StructType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.TimestampType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,TimestampType,Class,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,pyspark,Module,11,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,import pyspark,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,SparkContext,Class,14,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,from pyspark import SparkContext,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,SparkConf,Class,15,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,from pyspark.conf import SparkConf,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,DataFrame,Class,16,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,SparkSession,Class,16,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,F,Module,17,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,from pyspark.sql import functions as F,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,T,Module,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,from pyspark.sql import types as T,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,Window,Class,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.version.__version__,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotDefined,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,8,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf.setMaster,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.setAppName,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,10,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,11,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,12,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,13,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,14,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,15,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,16,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,17,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data-cerdas-november"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""12"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.s_project.hue_project"").set(""spark.driver.maxResultSize"", ""8g"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.context.SparkContext,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,1,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,8,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext.getOrCreate,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,1,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,8,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""conf"",""ParamType"":""SparkConf""}]",Python
pyspark.sql.session.SparkSession,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,SparkSession(sc),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,8,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.context.SparkContext"",""ParamType"":""SparkContext""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,5,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,5,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.pow,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,5,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sin,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,5,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.cos,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.cos,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.pow,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.sin,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.atan2,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,8,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,8,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""float"",""ParamType"":""float""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.udf(h3.latlng_to_cell),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,F.udf(h3.latlng_to_cell),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,staypoint.filter(F.col('event_date') <= end_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,staypoint.filter(F.col('event_date') >= start_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,21,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint.filter(F.substring('hash_msisdn', - 1, 1) == suffix)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,21,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('hash_msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,21,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('hash_msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,staypoint.filter(( F.col('latitude').isNotNull() ) & ( F.col('longitude').isNotNull() )),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,22,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,24,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,inbound.filter(F.col('event_date') <= end_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,24,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,24,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,inbound.filter(F.col('event_date') >= start_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('event_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound.filter(F.substring('imsi', - 1, 1) == suffix)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('imsi', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('imsi', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,inbound.filter(( F.col('latitude').isNotNull() ) & ( F.col('longitude').isNotNull() )),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('latitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('longitude').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,31,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,32,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,33,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,34,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,34,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,34,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,34,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,34,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,35,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,35,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,35,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,35,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,35,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,36,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,37,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,37,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(operator_name),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,37,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(operator_name),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,38,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,38,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit('INDONESIA'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,38,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit('INDONESIA'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,39,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,39,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_date(col('end_datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,39,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('end_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,39,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_date(col('end_datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.month(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.year(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.format_string,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.month,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.month(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.year,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,40,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.year(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,41,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,41,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,41,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,42,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,42,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,43,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,43,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,44,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,44,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,45,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,46,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"staypoint\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('start_time', 'start_datetime')\ .withColumnRenamed('end_time', 'end_datetime')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'start_datetime', 'end_datetime', 'longitude', 'latitude'])\ .withColumn('operator', f.lit(operator_name))\ .withColumn('country', f.lit('INDONESIA'))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .drop('event_date')\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,48,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,48,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,48,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.currentRow,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window.unboundedPreceding,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.rowsBetween,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,49,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,50,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,50,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,50,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.currentRow,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window.unboundedPreceding,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.rowsBetween,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,51,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement']).orderBy(['datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,52,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement', 'spot_movement'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,52,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'stop_movement', 'spot_movement'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,55,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,56,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,57,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,58,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,58,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,58,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,58,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,58,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('latitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,59,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,59,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,59,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,59,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude').cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,59,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('longitude').cast('double'), 5)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,60,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,61,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,61,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,61,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,61,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,62,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,62,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,62,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,62,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,63,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('datetime').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,63,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,63,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('datetime').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,63,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('datetime').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,64,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,64,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.unix_timestamp('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,64,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.unix_timestamp('lag_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,64,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,f.unix_timestamp('datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,64,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,f.unix_timestamp('lag_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,65,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,65,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('time_interval') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,65,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('time_interval'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,65,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('time_interval') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,66,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,66,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('lag_latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,66,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('lag_longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,66,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,66,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,67,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,67,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('distance') / ( col('time_interval') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,67,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('distance'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,67,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('time_interval'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,67,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('distance') / ( col('time_interval') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,68,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('speed') >= 10.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,68,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,68,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('speed') >= 10.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,68,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('speed'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,68,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('speed') >= 10.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,69,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('speed_raise').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,69,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,69,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('speed_raise').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,69,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('speed_raise').over(part0_1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('lag_speed_raise'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('speed_raise'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,70,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,71,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,71,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,71,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,71,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('time_interval_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,71,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('max_duration'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('speed_gap'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,72,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,73,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('stop_gap').over(part0_2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,73,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,73,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('stop_gap').over(part0_2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,73,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('stop_gap').over(part0_2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,74,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,74,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,74,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lit(10),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,75,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,75,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,76,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,76,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,77,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,77,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,78,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('h3_10').over(part0_3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,78,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,78,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('h3_10').over(part0_3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,78,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('h3_10').over(part0_3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('lag_h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,79,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,80,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('spot_gap').over(part0_4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,80,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,80,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('spot_gap').over(part0_4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,80,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('spot_gap').over(part0_4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('longitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.cos,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.cos,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('longitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,81,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(f.radians('longitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.cos,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.cos(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sin,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,82,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(f.radians('longitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,83,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,83,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,83,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,83,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.radians('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sin,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,83,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sin(f.radians('latitude')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,84,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.count('spot_movement').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,84,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,84,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.count('spot_movement').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,84,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.count('spot_movement').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,85,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('x').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,85,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,85,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('x').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,85,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,85,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('x').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,86,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('y').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,86,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,86,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('y').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,86,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,86,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('y').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,87,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('z').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,87,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,87,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('z').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,87,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,87,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('z').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,88,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,88,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,88,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('x_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,88,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('y_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,88,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(F.col('z_total'), F.col('sqrt'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('sqrt'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('z_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.degrees(f.atan2(F.col('z_total'), F.col('sqrt')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.atan2,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(F.col('z_total'), F.col('sqrt'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('sqrt'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('z_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.degrees,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.degrees(f.atan2(F.col('z_total'), F.col('sqrt')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,89,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(F.col('y_total'), F.col('x_total'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('x_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('y_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.degrees(f.atan2(F.col('y_total'), F.col('x_total')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.atan2,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.atan2(F.col('y_total'), F.col('x_total'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('x_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('y_total'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.degrees,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.degrees(f.atan2(F.col('y_total'), F.col('x_total')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,90,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,91,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.min('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,91,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,91,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.min('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.min,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,91,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.min('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,92,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.max('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,92,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,92,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.max('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,92,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.max('datetime').over(part0_5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,93,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,93,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_date(col('end_datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,93,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('end_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,93,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_date(col('end_datetime')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.month(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.year(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.format_string,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,False,False,NotSupported,"f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.month,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.month(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.year,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,94,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.year(col('event_month')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,95,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,95,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.lit(operator_name),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,95,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.lit(operator_name),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,96,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"inbound\ .withColumnRenamed('imsi', 'msisdn')\ .withColumnRenamed('hash_msisdn', 'msisdn')\ .withColumnRenamed('msisdn_encrypted', 'msisdn')\ .withColumn('latitude', f.round(col('latitude').cast('double'), 5))\ .withColumn('longitude', f.round(col('longitude').cast('double'), 5))\ .dropDuplicates(['msisdn', 'datetime', 'longitude', 'latitude'])\ .withColumn('lag_latitude', f.lag('latitude').over(part0_1))\ .withColumn('lag_longitude', f.lag('longitude').over(part0_1))\ .withColumn('lag_datetime', f.lag('datetime').over(part0_1))\ .withColumn('time_interval', f.unix_timestamp('datetime') - f.unix_timestamp('lag_datetime'))\ .withColumn('time_interval_hour', f.round(col('time_interval') / 3600, 2))\ .withColumn('distance', haversine_distance(col('lag_longitude'), col('lag_latitude'), col('longitude'), col('latitude')))\ .withColumn('speed', f.round(col('distance') / ( col('time_interval') / 3600 ), 2))\ .withColumn('speed_raise', f.when(col('speed') >= 10.0, 1).otherwise(0))\ .withColumn('lag_speed_raise', f.lag('speed_raise').over(part0_1))\ .withColumn('speed_gap', f.when(col('lag_speed_raise') != col('speed_raise'), 1).otherwise(0))\ .withColumn('max_duration', f.when(col('time_interval_hour') >= 8.0, 1).otherwise(0))\ .withColumn('stop_gap', f.when(( col('speed_gap') == 1 ) | ( col('max_duration') == 1 ), 1).otherwise(0))\ .withColumn('stop_movement', f.sum('stop_gap').over(part0_2))\ .withColumn('h3_10', geo_to_h3('latitude', 'longitude', f.lit(10)))\ .withColumn('h3_9', to_parent_res9_udf(col('h3_10')))\ .withColumn('h3_8', to_parent_res8_udf(col('h3_9')))\ .withColumn('h3_7', to_parent_res7_udf(col('h3_8')))\ .withColumn('lag_h3_10', f.lag('h3_10').over(part0_3))\ .withColumn('spot_gap', f.when(( col('lag_h3_10') != col('h3_10') ), 1).otherwise(0))\ .withColumn('spot_movement', f.sum('spot_gap').over(part0_4))\ .withColumn('x', f.cos(f.radians('latitude')) * f.cos(f.radians('longitude')))\ .withColumn('y', f.cos(f.radians('latitude')) * f.sin(f.radians('longitude')))\ .withColumn('z', f.sin(f.radians('latitude')))\ .withColumn('total', f.count('spot_movement').over(part0_5))\ .withColumn('x_total', f.sum('x').over(part0_5) / col('total'))\ .withColumn('y_total', f.sum('y').over(part0_5) / col('total'))\ .withColumn('z_total', f.sum('z').over(part0_5) / col('total'))\ .withColumn('sqrt', f.sqrt(col('x_total') * col('x_total') + col('y_total') * col('y_total')))\ .withColumn('centroid_lat', f.round(f.degrees(f.atan2(F.col('z_total'), F.col('sqrt'))), 6))\ .withColumn('centroid_lon', f.round(f.degrees(f.atan2(F.col('y_total'), F.col('x_total'))), 6))\ .withColumn('start_datetime', f.min('datetime').over(part0_5))\ .withColumn('end_datetime', f.max('datetime').over(part0_5))\ .withColumn('event_month', f.to_date(col('end_datetime')))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('event_month')), f.month(col('event_month'))))\ .withColumn('operator', F.lit(operator_name))\ .select(['msisdn', 'latitude', 'longitude', 'start_datetime', 'end_datetime', 'operator', 'country', 'event_month', 'h3_7', 'h3_9', 'h3_10'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.columns,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,98,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,inbound.columns,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,98,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,inbound.dropDuplicates(inbound.columns),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""typing.List[str]"",""ParamType"":""List""}]",Python
pyspark.sql.dataframe.DataFrame.union,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,100,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,staypoint.union(inbound),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,106,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,109,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,111,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"lau.withColumn('h3_7', to_parent_res7_udf(col('h3_10')))\ .drop('h3_10')\ .dropDuplicates(['h3_7'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,111,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,112,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"lau.withColumn('h3_7', to_parent_res7_udf(col('h3_10')))\ .drop('h3_10')\ .dropDuplicates(['h3_7'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,113,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"lau.withColumn('h3_7', to_parent_res7_udf(col('h3_10')))\ .drop('h3_10')\ .dropDuplicates(['h3_7'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,116,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.lpad(col(""kdkab""), 2, ""0"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,116,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"col(""kdkab"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.lpad,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,116,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.lpad(col(""kdkab""), 2, ""0"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,117,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.lpad(col(""kdkec""), 3, ""0"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,117,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"col(""kdkec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.lpad,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,117,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.lpad(col(""kdkec""), 3, ""0"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,118,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.concat_ws('|', 'kdprov', 'kdkab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.concat_ws,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,118,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"f.concat_ws('|', 'kdprov', 'kdkab')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,119,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.concat_ws('|', 'kdprov', 'kdkab', 'kdkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.concat_ws,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,119,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"f.concat_ws('|', 'kdprov', 'kdkab', 'kdkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,123,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw.join(lau, ['h3_7'], 'left')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,126,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('activity_kab', f.col('kab'))\ .withColumn('activity_kec', f.col('kec'))\ .withColumn('activity_prov', f.substring('kab', 1, 2))\ .withColumn('activity_prov', ( f.col('activity_prov').cast('int') ).cast('string'))\ .drop(""kab"", ""kec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,126,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,126,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,127,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('activity_kab', f.col('kab'))\ .withColumn('activity_kec', f.col('kec'))\ .withColumn('activity_prov', f.substring('kab', 1, 2))\ .withColumn('activity_prov', ( f.col('activity_prov').cast('int') ).cast('string'))\ .drop(""kab"", ""kec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,127,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,127,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('kec'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,128,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('activity_kab', f.col('kab'))\ .withColumn('activity_kec', f.col('kec'))\ .withColumn('activity_prov', f.substring('kab', 1, 2))\ .withColumn('activity_prov', ( f.col('activity_prov').cast('int') ).cast('string'))\ .drop(""kab"", ""kec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,128,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.substring('kab', 1, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,128,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.substring('kab', 1, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,129,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,( f.col('activity_prov').cast('int') ).cast('string'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,129,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('activity_prov').cast('int'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,129,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('activity_kab', f.col('kab'))\ .withColumn('activity_kec', f.col('kec'))\ .withColumn('activity_prov', f.substring('kab', 1, 2))\ .withColumn('activity_prov', ( f.col('activity_prov').cast('int') ).cast('string'))\ .drop(""kab"", ""kec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,129,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('activity_prov').cast('int'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,129,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.col('activity_prov').cast('int'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,130,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('activity_kab', f.col('kab'))\ .withColumn('activity_kec', f.col('kec'))\ .withColumn('activity_prov', f.substring('kab', 1, 2))\ .withColumn('activity_prov', ( f.col('activity_prov').cast('int') ).cast('string'))\ .drop(""kab"", ""kec"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,133,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw.join(jak_poi, ['h3_10'], 'left')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .withColumn('h3_7_poi', f.when(col('poi_id').isNotNull(), col('poi_id'))\ .otherwise(col('h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id'))\ .otherwise(col('h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('poi_id'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,137,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id'))\ .otherwise(col('h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,138,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id'))\ .otherwise(col('h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,138,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,142,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":null},{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.__getattr__,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,143,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,usual_env.home_kec,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,145,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,146,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,146,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,147,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql._typing.UserDefinedFunctionLike"",""ParamType"":""UserDefinedFunctionLike""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,147,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_8'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,148,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,149,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumnRenamed,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,150,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"usual_env\ .join(lau_ref.select(['idkec', 'nmkab', 'nmkec']), usual_env.home_kec == lau_ref.idkec, how = 'left')\ .drop('idkec')\ .withColumn('home_h3_7', to_parent_res7_udf(col('home_h3_8')))\ .withColumn('work_h3_7', to_parent_res7_udf(col('work_h3_8')))\ .drop('home_h3_8', 'work_h3_8')\ .withColumnRenamed('nmkab', 'home_nmkab')\ .withColumnRenamed('nmkec', 'home_nmkec')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,157,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,158,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.isNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_kab').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_kab').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,162,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.isNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_kab').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_kab'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_kab').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,163,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.isNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,164,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.isNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7').isNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,165,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,166,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"master_raw\ .join(usual_env, ['msisdn', 'event_month'], 'left')\ .select(['event_month', 'msisdn', 'home_kab', 'home_nmkab', 'home_kec', 'home_nmkec', 'work_kab', 'home_h3_7', 'work_h3_7', 'operator', 'country', 'latitude', 'longitude', 'nmprov', 'nmkab', 'nmkec', 'activity_prov', 'activity_kab', 'activity_kec', 'h3_10', 'h3_9', 'h3_7', 'h3_7_poi', 'poi_id', 'poi', 'start_datetime', 'end_datetime'])\ .withColumn('home_kab', f.when(( col('home_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_kab')))\ .withColumn('work_kab', f.when(( col('work_kab').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_kab')))\ .withColumn('home_h3_7', f.when(( col('home_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('home_h3_7')))\ .withColumn('work_h3_7', f.when(( col('work_h3_7').isNull() ) & ( col('country') != 'INDONESIA' ), col('country')).otherwise(col('work_h3_7')))\ .withColumn('country', f.upper(col('country')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,166,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.upper(col('country')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,166,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.upper,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,166,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.upper(col('country')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,169,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['start_datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,169,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['start_datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,169,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.partitionBy(['msisdn']).orderBy(['start_datetime']),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,174,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,174,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,174,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,mobility\ .filter(col('home_h3_7').isNotNull() & col('work_h3_7').isNotNull())\ .filter(col('h3_7').isNotNull()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,174,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,174,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,175,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,175,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,mobility\ .filter(col('home_h3_7').isNotNull() & col('work_h3_7').isNotNull())\ .filter(col('h3_7').isNotNull()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,175,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_7').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,179,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,179,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,179,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,179,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('latitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,180,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,180,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,180,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,180,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('longitude').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,181,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('end_datetime').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,181,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,181,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('end_datetime').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,181,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('end_datetime').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,182,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,182,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,182,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,182,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('prev_latitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,182,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('prev_longitude'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,183,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,183,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.unix_timestamp('prev_end_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,183,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.unix_timestamp('start_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,183,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,f.unix_timestamp('prev_end_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.unix_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,183,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,f.unix_timestamp('start_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,184,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,184,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('distance') / ( col('time_seconds') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,184,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('distance'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,184,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('time_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,184,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('distance') / ( col('time_seconds') / 3600 ), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,185,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('prev_latitude', f.lag('latitude').over(part1))\ .withColumn('prev_longitude', f.lag('longitude').over(part1))\ .withColumn('prev_end_datetime', f.lag('end_datetime').over(part1))\ .withColumn('distance', haversine_distance(col('prev_longitude'), col('prev_latitude'), col('longitude'), col('latitude')))\ .withColumn('time_seconds', f.unix_timestamp('start_datetime') - f.unix_timestamp('prev_end_datetime'))\ .withColumn('speed', f.round(col('distance') / ( col('time_seconds') / 3600 ), 2))\ .filter(col('speed') <= max_speed)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,185,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('speed'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.currentRow,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window.unboundedPreceding,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Variable,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.rowsBetween,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,189,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,190,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,190,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,197,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,197,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,197,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,197,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('country'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,197,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,198,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,198,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('time_seconds') / 3600, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,198,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('time_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,198,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('time_seconds') / 3600, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('interval_hours'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,199,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,2,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('home_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('work_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,200,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,201,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,201,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,201,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,201,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lag('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,202,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lead('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,202,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,202,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lead('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lead,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,202,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.lead('flag_env').over(part1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('flag_env') != col('prev_env'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('flag_env') != col('prev_env'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('flag_env'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('prev_env'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,203,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('flag_env') != col('prev_env'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('trip_gap').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('trip_gap').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('trip_gap').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,204,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,205,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,205,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('flag_move_env').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,205,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('flag_move_env').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,205,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.sum('flag_move_env').over(part2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,206,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.first('start_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,206,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,206,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.first('start_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.first,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,206,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,f.first('start_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,207,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.last('end_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,207,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,207,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.last('end_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.last,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,207,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,f.last('end_datetime').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('end_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('start_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('end_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('start_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('end_trip_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('start_trip_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('end_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,208,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,f.to_timestamp(col('start_trip_datetime')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,209,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,209,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('interval_trip_seconds') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,209,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('interval_trip_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,209,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.round(col('interval_trip_seconds') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,210,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_0\ .withColumn('category', f.when(col('country') != 'INDONESIA', 'Inbound').otherwise('Domestic'))\ .withColumn('interval_hours', f.round(col('time_seconds') / 3600, 1))\ .withColumn('trip_gap', f.when(( col('category') == 'Inbound' ) & ( col('interval_hours') > trip_gap_inbound ), 1).otherwise(0))\ .withColumn('flag_env', f.when(( col('h3_7') == col('home_h3_7') ) | ( col('h3_7') == col('work_h3_7') ), 1).otherwise(0))\ .withColumn('prev_env', f.lag('flag_env').over(part1))\ .withColumn('next_env', f.lead('flag_env').over(part1))\ .withColumn('flag_move_env', f.when(col('flag_env') != col('prev_env'), 1).otherwise(0))\ .withColumn('move_trip', f.when(col('category') == 'Inbound', f.sum('trip_gap').over(part2))\ .otherwise(f.sum('flag_move_env').over(part2)))\ .withColumn('start_trip_datetime', f.first('start_datetime').over(part3))\ .withColumn('end_trip_datetime', f.last('end_datetime').over(part3))\ .withColumn('interval_trip_seconds', f.to_timestamp(col('end_trip_datetime')).cast('double') - f.to_timestamp(col('start_trip_datetime')).cast('double'))\ .withColumn('interval_trip_hour', f.round(col('interval_trip_seconds') / 3600, 2))\ .withColumn('interval_trip_day', f.datediff(col('end_trip_datetime'), col('start_trip_datetime')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,210,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.datediff(col('end_trip_datetime'), col('start_trip_datetime'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,210,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('end_trip_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,210,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('start_trip_datetime'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.datediff,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,210,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,"f.datediff(col('end_trip_datetime'), col('start_trip_datetime'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,214,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,mobility_1_trip\ .filter(col('interval_trip_hour') >= trip_hour_mark),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,214,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,col('interval_trip_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,216,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_1_trip.withColumn('suffix', F.substring('msisdn', - 1, 1))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,216,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,216,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring('msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,217,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_1_trip.withColumn('execute_month', F.substring(F.lit(end_date), 1, 7))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,217,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.lit(end_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,217,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring(F.lit(end_date), 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,217,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.lit(end_date),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.substring,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,217,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.substring(F.lit(end_date), 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,11,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.udf(to_parent_res9, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"f.udf(to_parent_res9, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.udf(to_parent_res8, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"f.udf(to_parent_res8, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"f.udf(to_parent_res7, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"f.udf(to_parent_res7, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Class,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pe_bps.wisnus_sample5_staypoint_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pe_bps.wisnus_sample5_staypoint_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.inbound_sample5_lbs_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,4,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.inbound_sample5_lbs_2024'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,6,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int.data_cerdas_ue_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,9,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.createDataFrame(jak_zone),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,12,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.createDataFrame(jak_poi),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,15,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.createDataFrame(lau_ref),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.createDataFrame(event),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,21,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.createDataFrame(cfd),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,23,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pe_bps.indonesia_h3_38prov'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,23,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pe_bps.indonesia_h3_38prov'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,24,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"lau.select(['h3_10', 'kab', 'kec', 'nmprov', 'nmkab', 'nmkec'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,15,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"mobility_trip.repartition(100).write.mode('append').partitionBy('execute_month', 'event_month', 'suffix').saveAsTable('pnt_bps_int_stg.data_cerdas_mobility_trip_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.write,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_trip.repartition(100).write.mode('append').partitionBy('execute_month', 'event_month', 'suffix').saveAsTable('pnt_bps_int_stg.data_cerdas_mobility_trip_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameWriter.mode,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"mobility_trip.repartition(100).write.mode('append').partitionBy('execute_month', 'event_month', 'suffix').saveAsTable('pnt_bps_int_stg.data_cerdas_mobility_trip_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.partitionBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Rename,"mobility_trip.repartition(100).write.mode('append').partitionBy('execute_month', 'event_month', 'suffix').saveAsTable('pnt_bps_int_stg.data_cerdas_mobility_trip_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.saveAsTable,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Transformation,"mobility_trip.repartition(100).write.mode('append').partitionBy('execute_month', 'event_month', 'suffix').saveAsTable('pnt_bps_int_stg.data_cerdas_mobility_trip_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.sql(q).toPandas()[- 20:],0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.sql,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.sql(q).toPandas()[- 20:],0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,1,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,1,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('home_nmkab').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,df.filter(F.col('home_nmkab').isNotNull()),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('home_nmkab').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,2,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('home_nmkab').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.limit,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,df.limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,3,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,df.limit(10).toPandas(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,23,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,F,Module,1,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,from pyspark.sql import functions as F,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,df.filter(F.col('execute_month') == execute_month),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,7,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.count,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,11,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,df.count(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,16,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('suffix')\ .count()\ .orderBy('suffix')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.group.GroupedData.count,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,17,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('suffix')\ .count()\ .orderBy('suffix')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,18,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('suffix')\ .count()\ .orderBy('suffix')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('suffix')\ .count()\ .orderBy('suffix')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.round(F.col('count') * 100 / total_rows, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,19,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.round(F.col('count') * 100 / total_rows, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.show,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,20,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('suffix')\ .count()\ .orderBy('suffix')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,24,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('event_month')\ .count()\ .orderBy('event_month')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.count,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,25,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('event_month')\ .count()\ .orderBy('event_month')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.orderBy,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,26,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('event_month')\ .count()\ .orderBy('event_month')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('event_month')\ .count()\ .orderBy('event_month')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Module,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.round(F.col('count') * 100 / total_rows, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,F.col('count'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.round,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,27,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"F.round(F.col('count') * 100 / total_rows, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.show,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,28,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,"df.groupBy('event_month')\ .count()\ .orderBy('event_month')\ .withColumn('percentage', F.round(F.col('count') * 100 / total_rows, 2))\ .show()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,33,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_2. Pipeline Periodically - Mobility Trip.ipynb,1,,Function,33,notebook.Sample_2. Pipeline Periodically - Mobility Trip,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,f,Module,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,import pyspark.sql.functions as f,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,SparkSession,Class,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,from pyspark.sql import SparkSession,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,col,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,lag,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lead,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,lead,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.functions import col, lag, lead",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,Window,Class,8,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DateType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,DateType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.DoubleType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,DoubleType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.IntegerType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,IntegerType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StringType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,StringType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructField,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,StructField,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.StructType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,StructType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types.TimestampType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,TimestampType,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType, StringType, DateType",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,4,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,pyspark,Module,11,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,import pyspark,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,SparkContext,Class,14,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,from pyspark import SparkContext,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,SparkConf,Class,15,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,from pyspark.conf import SparkConf,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,DataFrame,Class,16,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,SparkSession,Class,16,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,"from pyspark.sql import DataFrame, SparkSession",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,F,Module,17,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,from pyspark.sql import functions as F,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.types,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,T,Module,18,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,from pyspark.sql import types as T,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,Window,Class,19,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,from pyspark.sql.window import Window,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.version.__version__,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Variable,25,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotDefined,pyspark.__version__,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,5,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,8,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.conf.SparkConf.setMaster,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,9,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.setAppName,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,10,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,11,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,12,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,13,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,14,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,15,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,16,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,17,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.conf.SparkConf.set,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,21,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"SparkConf().setMaster(""yarn"").setAppName(""data_cerdas_tourism"").set(""spark.dynamicAllocation.maxExecutors"", ""100"").set(""spark.dynamicAllocation.minExecutors"", ""1"").set(""spark.executor.cores"", ""10"").set(""spark.executor.memory"", ""64g"").set(""spark.sql.shuffle.partitions"", ""7000"").set(""spark.yarn.queue"", ""root.pnt.hui_pnt_bpsint"").set(""spark.yarn.appMasterEnv.PYSPARK_PYTHON"", ""./env/bin/python"", ).set(""spark.yarn.dist.archives"", ""hdfs://nsdiscovery/warehouse/tablespace/external/hive/pnt_bps_int.db/envs/mobility_310.tar.gz#env"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,6,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.context.SparkContext,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.context.SparkContext.getOrCreate,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,SparkContext.getOrCreate(conf = conf),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""conf"",""ParamType"":""SparkConf""}]",Python
pyspark.sql.session.SparkSession,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,SparkSession(sc),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,7,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.context.SparkContext"",""ParamType"":""SparkContext""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat2 - lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.radians,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lon2 - lon1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.pow,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.pow(f.sin(dlat / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sin,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sin(dlat / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.cos,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.cos(f.radians(lat1)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.cos,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.cos(f.radians(lat2)),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.pow,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.pow(f.sin(dlon / 2), 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.radians,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat1),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions.radians,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.radians(lat2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions.sin,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sin(dlon / 2),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.atan2,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.atan2(f.sqrt(a), f.sqrt(1 - a))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sqrt(1 - a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sqrt,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sqrt(a),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,8,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.round,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,8,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.round(R * c, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""float"",""ParamType"":""float""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.udf(h3.latlng_to_cell),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,F.udf(h3.latlng_to_cell),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,9,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.udf(to_parent_res9, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"f.udf(to_parent_res9, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.udf(to_parent_res8, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"f.udf(to_parent_res8, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.udf(to_parent_res7, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.udf,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"f.udf(to_parent_res7, StringType())",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol"",""ParamType"":""_SpecialErrorPackage.Snowflake.SnowConvert.Python.SymbolTable.PyUnresolvedSymbol""},{""ParamName"":""pyspark.sql.types.StringType"",""ParamType"":""StringType""}]",Python
pyspark.sql.types.StringType,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,StringType(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,10,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_1_trip.filter(F.col('execute_month') == execute_month),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_1_trip.filter(F.col('suffix') == suffix),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('suffix'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('suffix'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.currentRow,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Variable,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window.unboundedPreceding,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Variable,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.rowsBetween,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,7,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,9,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.currentRow,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Variable,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.currentRow,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window.unboundedPreceding,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Variable,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,Window.unboundedPreceding,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.rowsBetween,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,10,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip']).orderBy(['start_datetime']).rowsBetween(Window.unboundedPreceding, Window.currentRow)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,12,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,12,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,13,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_poi'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,13,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_poi'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,14,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_event'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,14,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_event'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,15,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_cfd'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,15,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_cfd'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,23,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,24,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('poi_id').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('poi_id').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,25,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('poi_id').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('prev_poi_id'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,26,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,27,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != '-', col('poi_id')).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""None"",""ParamType"":""None""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,27,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,27,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != '-', col('poi_id')).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,2,,Function,27,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,27,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id') != '-', col('poi_id')).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,28,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_poi').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,28,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,28,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_poi').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,28,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_poi').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""None"",""ParamType"":""None""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.first,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.first('start_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,29,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.isNotNull,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""None"",""ParamType"":""None""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('poi_id').isNotNull(),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.last,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.last('end_datetime').over(part7),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,30,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,31,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_poi')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,32,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,32,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.round(col('interval_visit_seconds_poi') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,32,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_seconds_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.round,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,32,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.round(col('interval_visit_seconds_poi') / 3600, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""any"",""ParamType"":""any""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,33,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,33,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,33,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,33,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.datediff,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,33,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,"f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,34,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_1_trip\ .drop('h3_10')\ .withColumn('poi_id', f.when(col('poi_id').isNotNull(), col('poi_id')).otherwise('-'))\ .withColumn('prev_poi_id', f.lag('poi_id').over(part4))\ .withColumn('flag_move_poi', f.when(col('poi_id') != col('prev_poi_id'), 1).otherwise(0))\ .withColumn('poi_id', f.when(col('poi_id') != '-', col('poi_id')).otherwise(None))\ .withColumn('move_visit_poi', f.sum('flag_move_poi').over(part5))\ .withColumn('first_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.first('start_datetime').over(part7)).otherwise(None))\ .withColumn('last_datetime_visit_poi', f.when(col('poi_id').isNotNull(), f.last('end_datetime').over(part7)).otherwise(None))\ .withColumn('interval_visit_seconds_poi', f.to_timestamp(col('last_datetime_visit_poi')).cast('double') - f.to_timestamp(col('first_datetime_visit_poi')).cast('double'))\ .withColumn('interval_visit_hour_poi', f.round(col('interval_visit_seconds_poi') / 3600, 2))\ .withColumn('interval_visit_day_poi', f.datediff(col('last_datetime_visit_poi'), col('first_datetime_visit_poi')))\ .withColumn('flag_visit_poi', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,34,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,34,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('flag_env'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,34,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,35,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds_poi') >= ( 3600 * visit_hour_mark ) ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,35,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_seconds_poi'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,40,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,40,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,40,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,40,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('prev_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,41,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,42,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,42,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,42,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,42,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,43,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,43,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,43,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.first,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,43,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,44,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,44,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,44,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.last,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,44,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,45,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,46,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,46,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,46,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,46,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.datediff,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,46,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,"f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,47,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,47,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,47,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('flag_env'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,47,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,48,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_day'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,48,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,49,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,51,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))\ .withColumn('flag_visit', f.when(( col('flag_env') != 1 ) & ( ( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) | ( col('interval_visit_day') > 0 ) ), 1).otherwise(0))\ .filter(col('flag_visit') == 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,51,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('flag_visit'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,55,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,55,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,55,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,55,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_7').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('prev_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,56,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,57,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,57,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,57,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,57,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_7').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,58,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,58,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,58,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.first,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,58,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.first('start_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,59,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,59,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,59,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.last,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,59,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.last('end_datetime').over(part6),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,60,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,61,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('prev_h3_7', f.lag('h3_7').over(part4))\ .withColumn('flag_move_h3_7', f.when(col('h3_7') != col('prev_h3_7'), 1).otherwise(0))\ .withColumn('move_visit', f.sum('flag_move_h3_7').over(part5))\ .withColumn('first_datetime_visit_h3_7', f.first('start_datetime').over(part6))\ .withColumn('last_datetime_visit_h3_7', f.last('end_datetime').over(part6))\ .withColumn('interval_visit_seconds', f.to_timestamp(col('last_datetime_visit_h3_7')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_7')).cast('double'))\ .withColumn('interval_visit_day', f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,61,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,61,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,61,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.datediff,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,61,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,"f.datediff(col('last_datetime_visit_h3_7'), col('first_datetime_visit_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,65,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('activity_prov') == ""31"", 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,65,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('flag_jkt', f.when(col('activity_prov') == ""31"", 1).otherwise(0))\ .withColumn('visit_jkt', f.sum('flag_jkt').over(part3))\ .filter(col('visit_jkt') > 0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,65,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('activity_prov') == ""31"", 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,65,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('activity_prov'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,65,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('activity_prov') == ""31"", 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,66,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_jkt').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,66,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('flag_jkt', f.when(col('activity_prov') == ""31"", 1).otherwise(0))\ .withColumn('visit_jkt', f.sum('flag_jkt').over(part3))\ .filter(col('visit_jkt') > 0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,66,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_jkt').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,66,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_jkt').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,67,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_2_visit\ .withColumn('flag_jkt', f.when(col('activity_prov') == ""31"", 1).otherwise(0))\ .withColumn('visit_jkt', f.sum('flag_jkt').over(part3))\ .filter(col('visit_jkt') > 0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,67,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('visit_jkt'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,74,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,74,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,80,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_2_visit\ .filter(col('flag_env') == 0)\ .filter(col('interval_trip_day') < max_day_trip),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,80,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('flag_env'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,81,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_2_visit\ .filter(col('flag_env') == 0)\ .filter(col('interval_trip_day') < max_day_trip),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,81,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_trip_day'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,85,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('interval_visit_seconds').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,85,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism\ .withColumn('max_los_visit', f.max('interval_visit_seconds').over(part3))\ .withColumn('destination_category', f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,85,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('interval_visit_seconds').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,85,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('interval_visit_seconds').over(part3),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,86,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism\ .withColumn('max_los_visit', f.max('interval_visit_seconds').over(part3))\ .withColumn('destination_category', f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,87,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,2,,Function,87,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,87,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('max_los_visit'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,87,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,88,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,88,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_seconds'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,89,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ) & ( col('interval_visit_seconds') == col('max_los_visit') ), 'main')\ .when(( col('interval_visit_seconds') >= ( 3600 * visit_hour_mark ) ), 'secondary')\ .otherwise('overstay_night')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,97,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism\ .filter(col('destination_category') == 'main')\ .select('event_month', 'msisdn', 'move_trip', col('h3_7').alias('main_dest_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,97,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('destination_category'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,98,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_7').alias('main_dest_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,98,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism\ .filter(col('destination_category') == 'main')\ .select('event_month', 'msisdn', 'move_trip', col('h3_7').alias('main_dest_h3_7'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,98,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_7').alias('main_dest_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.columns,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,99,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,main_dest.columns,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,99,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,main_dest.dropDuplicates(main_dest.columns),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""typing.List[str]"",""ParamType"":""List""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,103,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"main_dest\ .groupBy('event_month', 'msisdn', 'main_dest_h3_7')\ .agg(f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'))\ .withColumn('flag_circular_trip', f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,104,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,104,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,104,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,104,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"main_dest\ .groupBy('event_month', 'msisdn', 'main_dest_h3_7')\ .agg(f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'))\ .withColumn('flag_circular_trip', f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,105,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,105,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"main_dest\ .groupBy('event_month', 'msisdn', 'main_dest_h3_7')\ .agg(f.countDistinct('move_trip').alias('main_dest_h3_7_n_trip'))\ .withColumn('flag_circular_trip', f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,105,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,105,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('main_dest_h3_7_n_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,105,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('main_dest_h3_7_n_trip') >= mark_circular, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.columns,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,108,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_3_tourism.columns,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,109,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"mobility_3_tourism.alias('m3')\ .join(main_dest.alias('md'), ( col('m3.event_month') == col('md.event_month') ) & ( col('m3.msisdn') == col('md.msisdn') ) & ( col('m3.move_trip') == col('md.move_trip') ), how = 'inner')\ .select(col('m3.event_month'), col('m3.msisdn'), *cols, col('m3.move_trip'), 'main_dest_h3_7')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,110,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism.alias('m3')\ .join(main_dest.alias('md'), ( col('m3.event_month') == col('md.event_month') ) & ( col('m3.msisdn') == col('md.msisdn') ) & ( col('m3.move_trip') == col('md.move_trip') ), how = 'inner')\ .select(col('m3.event_month'), col('m3.msisdn'), *cols, col('m3.move_trip'), 'main_dest_h3_7')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,111,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,main_dest.alias('md'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,112,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,112,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('md.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,113,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,113,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('md.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,114,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.move_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,114,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('md.move_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,117,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_3_tourism.alias('m3')\ .join(main_dest.alias('md'), ( col('m3.event_month') == col('md.event_month') ) & ( col('m3.msisdn') == col('md.msisdn') ) & ( col('m3.move_trip') == col('md.move_trip') ), how = 'inner')\ .select(col('m3.event_month'), col('m3.msisdn'), *cols, col('m3.move_trip'), 'main_dest_h3_7')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""list"",""ParamType"":""list""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,117,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,117,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.move_trip'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,117,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m3.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.columns,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,119,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_4_tourism.columns,0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,120,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"mobility_4_tourism.alias('m4')\ .join(main_dest_count.alias('mdc'), ( col('m4.msisdn') == col('mdc.msisdn') ) & ( col('m4.event_month') == col('mdc.event_month') ) & ( col('m4.main_dest_h3_7') == col('mdc.main_dest_h3_7') ), how = 'left')\ .select(col('m4.event_month'), col('m4.msisdn'), *cols, col('m4.main_dest_h3_7'), 'main_dest_h3_7_n_trip', 'flag_circular_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,121,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism.alias('m4')\ .join(main_dest_count.alias('mdc'), ( col('m4.msisdn') == col('mdc.msisdn') ) & ( col('m4.event_month') == col('mdc.event_month') ) & ( col('m4.main_dest_h3_7') == col('mdc.main_dest_h3_7') ), how = 'left')\ .select(col('m4.event_month'), col('m4.msisdn'), *cols, col('m4.main_dest_h3_7'), 'main_dest_h3_7_n_trip', 'flag_circular_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,122,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,main_dest_count.alias('mdc'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,123,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,123,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('mdc.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,124,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,124,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('mdc.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,125,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.main_dest_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,125,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('mdc.main_dest_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,128,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism.alias('m4')\ .join(main_dest_count.alias('mdc'), ( col('m4.msisdn') == col('mdc.msisdn') ) & ( col('m4.event_month') == col('mdc.event_month') ) & ( col('m4.main_dest_h3_7') == col('mdc.main_dest_h3_7') ), how = 'left')\ .select(col('m4.event_month'), col('m4.msisdn'), *cols, col('m4.main_dest_h3_7'), 'main_dest_h3_7_n_trip', 'flag_circular_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""list"",""ParamType"":""list""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,128,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,128,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.main_dest_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,128,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('m4.msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,132,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,132,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,133,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.asc('interval_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.asc,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,133,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.asc('interval_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,133,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.asc('interval_visit_h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,133,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.asc('interval_visit_h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,133,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.asc('interval_visit_h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,134,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.desc('candidate_score_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.desc,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,134,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.desc('candidate_score_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,134,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_prev'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,134,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_prev'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,134,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_prev'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,135,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.desc('candidate_score_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.desc,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,135,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.desc('candidate_score_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,135,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_next'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,135,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_next'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.window.WindowSpec.orderBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,135,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'date']).orderBy(f.desc('candidate_score_next'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.window.Window,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Class,137,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_h9'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.window.Window.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,137,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"Window.partitionBy(['msisdn', 'move_trip', 'move_visit_h9'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,140,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,143,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_9').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,143,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,143,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_9').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lag,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,143,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lag('h3_9').over(part4),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('prev_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,144,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,145,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_9').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,145,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,145,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_9').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.sum,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,145,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.sum('flag_move_h3_9').over(part5),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,146,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,146,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,146,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.first('start_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.first,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,146,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.first('start_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,147,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,147,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(['msisdn', 'move_trip', 'move_visit', 'h3_7', 'h3_9', 'start_datetime', 'end_datetime'])\ .withColumn('prev_h3_9', f.lag('h3_9').over(part4))\ .withColumn('flag_move_h3_9', f.when(col('h3_9') != col('prev_h3_9'), 1).otherwise(0))\ .withColumn('move_visit_h9', f.sum('flag_move_h3_9').over(part5))\ .withColumn('first_datetime_visit_h3_9', f.first('start_datetime').over(part6b))\ .withColumn('last_datetime_visit_h3_9', f.last('end_datetime').over(part6b))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,147,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.last('end_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.last,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,147,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,f.last('end_datetime').over(part6b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,153,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,154,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list"",""ParamType"":""list""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.cast,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.to_timestamp,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,155,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,156,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,156,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_date(col('first_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,156,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,156,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_date(col('first_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,157,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,157,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_date(col('last_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,157,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.to_date,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,157,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.to_date(col('last_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,158,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,158,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_date'), col('end_date'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,158,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('end_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,158,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('start_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.sequence,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,158,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_date'), col('end_date'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,159,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,159,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.explode(col('date_seq')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,159,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('date_seq'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.explode,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,159,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.explode(col('date_seq')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,160,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,160,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.hour(col('first_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,160,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('first_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.hour,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,160,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.hour(col('first_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,161,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,161,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.hour(col('last_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,161,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.hour,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,161,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.hour(col('last_datetime_visit_h3_9')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,162,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_hour'), col('end_hour'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('end_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('end_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('start_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('start_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.sequence,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_hour'), col('end_hour'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,163,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(23),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_hour'), f.lit(23))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('start_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('start_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(23),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sequence,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,164,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(col('start_hour'), f.lit(23))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(0),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(f.lit(0), col('end_hour'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('end_date'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('end_hour'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(0),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sequence,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,165,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(f.lit(0), col('end_hour'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(0),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(23),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(f.lit(0), f.lit(23))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(0),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.lit(23),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.sequence,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,166,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.sequence(f.lit(0), f.lit(23))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,167,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.row_number().over(part3c),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,167,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,167,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.row_number().over(part3c),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.row_number,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,167,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.row_number().over(part3c),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,168,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,170,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,170,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})""))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.expr,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,170,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.size,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,170,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})""))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,172,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,174,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,174,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})""))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.expr,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,174,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.size,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,174,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})""))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.drop,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,176,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,177,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,177,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('prev_nhour_overstay'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,177,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('rank_spent_time_x10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,178,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,178,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('next_nhour_overstay'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,178,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('rank_spent_time_x10'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,179,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_prev').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,179,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,179,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_prev').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,179,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_prev').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,180,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_next').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,180,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,180,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_next').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.max,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,180,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max('candidate_score_next').over(part3b),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,181,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3d),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,181,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,181,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3d),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.rank,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,181,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3d),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,182,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,182,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,182,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('candidate_score_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,182,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('max_candidate_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,182,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,183,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('max_candidate_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,184,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('rank_within_date_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,185,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.over,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,186,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3e),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.window.WindowSpec"",""ParamType"":""WindowSpec""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,186,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,186,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3e),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.rank,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,186,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.rank().over(part3e),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,187,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(cols)\ .dropDuplicates(cols)\ .withColumn('interval_visit_h3_9', f.to_timestamp(col('last_datetime_visit_h3_9')).cast('double') - f.to_timestamp(col('first_datetime_visit_h3_9')).cast('double'))\ .withColumn('start_date', f.to_date(col('first_datetime_visit_h3_9')))\ .withColumn('end_date', f.to_date(col('last_datetime_visit_h3_9')))\ .withColumn('date_seq', f.sequence(col('start_date'), col('end_date')))\ .withColumn('date', f.explode(col('date_seq')))\ .withColumn('start_hour', f.hour(col('first_datetime_visit_h3_9')))\ .withColumn('end_hour', f.hour(col('last_datetime_visit_h3_9')))\ .withColumn('hour_seq', f.when(col('start_date') == col('end_date'), f.sequence(col('start_hour'), col('end_hour')))\ .when(col('date') == col('start_date'), f.sequence(col('start_hour'), f.lit(23)))\ .when(col('date') == col('end_date'), f.sequence(f.lit(0), col('end_hour')))\ .otherwise(f.sequence(f.lit(0), f.lit(23))))\ .withColumn('rank_spent_time_x10', f.row_number().over(part3c) * 10)\ .withColumn(""prev_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x <= {end_overstay_hours})"")))\ .withColumn(""next_nhour_overstay"", f.size(f.expr(f""filter(hour_seq, x -> x >= {start_overstay_hours})"")))\ .drop('date_seq', 'first_date_visit_h3_9', 'last_date_visit_h3_9')\ .withColumn(""candidate_score_prev"", col('rank_spent_time_x10') * ( col('prev_nhour_overstay') ))\ .withColumn(""candidate_score_next"", col('rank_spent_time_x10') * ( col('next_nhour_overstay') ))\ .withColumn('max_candidate_prev', f.max('candidate_score_prev').over(part3b))\ .withColumn('max_candidate_next', f.max('candidate_score_next').over(part3b))\ .withColumn('rank_within_date_prev', f.rank().over(part3d))\ .withColumn('overstay_h3_9_prev', f.when(( col('candidate_score_prev') == col('max_candidate_prev') ) & ( col('max_candidate_prev') > 0 ) & ( col('rank_within_date_prev') == 1 ), col('h3_9')))\ .withColumn('rank_within_date_next', f.rank().over(part3e))\ .withColumn('overstay_h3_9_next', f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,187,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,187,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('candidate_score_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,187,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('max_candidate_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,187,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(( col('candidate_score_next') == col('max_candidate_next') ) & ( col('max_candidate_next') > 0 ) & ( col('rank_within_date_next') == 1 ), col('h3_9'))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,188,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('max_candidate_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,189,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('rank_within_date_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,190,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,192,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay.groupBy('msisdn', 'date')\ .agg(f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'), f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'))\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_prev'), col('zone_id').alias('zone_id_prev'), col('zone_category').alias('zone_category_prev')]), ['overstay_h3_9_prev'], how = 'left')\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_next'), col('zone_id').alias('zone_id_next'), col('zone_category').alias('zone_category_next')]), ['overstay_h3_9_next'], how = 'left')\ .select(['msisdn', 'date', 'overstay_h3_9_prev', 'zone_id_prev', 'zone_category_prev', 'overstay_h3_9_next', 'zone_id_next', 'zone_category_next'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,193,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,193,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,193,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.max,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,193,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,193,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay.groupBy('msisdn', 'date')\ .agg(f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'), f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'))\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_prev'), col('zone_id').alias('zone_id_prev'), col('zone_category').alias('zone_category_prev')]), ['overstay_h3_9_prev'], how = 'left')\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_next'), col('zone_id').alias('zone_id_next'), col('zone_category').alias('zone_category_next')]), ['overstay_h3_9_next'], how = 'left')\ .select(['msisdn', 'date', 'overstay_h3_9_prev', 'zone_id_prev', 'zone_category_prev', 'overstay_h3_9_next', 'zone_id_next', 'zone_category_next'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,194,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,194,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,194,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.max,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,194,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,195,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay.groupBy('msisdn', 'date')\ .agg(f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'), f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'))\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_prev'), col('zone_id').alias('zone_id_prev'), col('zone_category').alias('zone_category_prev')]), ['overstay_h3_9_prev'], how = 'left')\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_next'), col('zone_id').alias('zone_id_next'), col('zone_category').alias('zone_category_next')]), ['overstay_h3_9_next'], how = 'left')\ .select(['msisdn', 'date', 'overstay_h3_9_prev', 'zone_id_prev', 'zone_category_prev', 'overstay_h3_9_next', 'zone_id_next', 'zone_category_next'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":null},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9').alias('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_category').alias('zone_category_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_id').alias('zone_id_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9').alias('overstay_h3_9_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_category').alias('zone_category_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,196,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_id').alias('zone_id_prev'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,199,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay.groupBy('msisdn', 'date')\ .agg(f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'), f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'))\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_prev'), col('zone_id').alias('zone_id_prev'), col('zone_category').alias('zone_category_prev')]), ['overstay_h3_9_prev'], how = 'left')\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_next'), col('zone_id').alias('zone_id_next'), col('zone_category').alias('zone_category_next')]), ['overstay_h3_9_next'], how = 'left')\ .select(['msisdn', 'date', 'overstay_h3_9_prev', 'zone_id_prev', 'zone_category_prev', 'overstay_h3_9_next', 'zone_id_next', 'zone_category_next'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":null},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9').alias('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_category').alias('zone_category_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_id').alias('zone_id_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('h3_9').alias('overstay_h3_9_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_category').alias('zone_category_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,200,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('zone_id').alias('zone_id_next'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,203,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay.groupBy('msisdn', 'date')\ .agg(f.max(col('overstay_h3_9_prev')).alias('overstay_h3_9_prev'), f.max(col('overstay_h3_9_next')).alias('overstay_h3_9_next'))\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_prev'), col('zone_id').alias('zone_id_prev'), col('zone_category').alias('zone_category_prev')]), ['overstay_h3_9_prev'], how = 'left')\ .join(jak_zone.select([col('h3_9').alias('overstay_h3_9_next'), col('zone_id').alias('zone_id_next'), col('zone_category').alias('zone_category_next')]), ['overstay_h3_9_next'], how = 'left')\ .select(['msisdn', 'date', 'overstay_h3_9_prev', 'zone_id_prev', 'zone_category_prev', 'overstay_h3_9_next', 'zone_id_next', 'zone_category_next'])",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,206,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(['msisdn', 'move_trip', 'move_visit', 'date'])\ .join(mobility_overstay_daily, ['msisdn', 'date'], how = 'left')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list[str]"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,207,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_overstay\ .select(['msisdn', 'move_trip', 'move_visit', 'date'])\ .join(mobility_overstay_daily, ['msisdn', 'date'], how = 'left')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.select,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,223,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(cols)\ .dropDuplicates(cols)\ .repartition(1000)\ .join(mobility_overstay.repartition(1000), ['msisdn', 'move_trip', 'move_visit'], how = 'right')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.dropDuplicates,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,224,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(cols)\ .dropDuplicates(cols)\ .repartition(1000)\ .join(mobility_overstay.repartition(1000), ['msisdn', 'move_trip', 'move_visit'], how = 'right')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""list"",""ParamType"":""list""}]",Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,225,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"mobility_4_tourism\ .select(cols)\ .dropDuplicates(cols)\ .repartition(1000)\ .join(mobility_overstay.repartition(1000), ['msisdn', 'move_trip', 'move_visit'], how = 'right')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.join,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,226,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_4_tourism\ .select(cols)\ .dropDuplicates(cols)\ .repartition(1000)\ .join(mobility_overstay.repartition(1000), ['msisdn', 'move_trip', 'move_visit'], how = 'right')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.dataframe.DataFrame"",""ParamType"":""DataFrame""},{""ParamName"":""list[str]"",""ParamType"":""list""},{""ParamName"":""how"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,227,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,mobility_overstay.repartition(1000),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.column.Column.otherwise,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,232,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('interval_visit_day') > 0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,232,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_5_tourism\ .withColumn('visit_overstay_night', f.when(col('interval_visit_day') > 0, 1).otherwise(0))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('last_datetime_visit_h3_7')), f.month(col('last_datetime_visit_h3_7'))))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,232,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('interval_visit_day') > 0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,232,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('interval_visit_day'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.when,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,232,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.when(col('interval_visit_day') > 0, 1).otherwise(0)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_5_tourism\ .withColumn('visit_overstay_night', f.when(col('interval_visit_day') > 0, 1).otherwise(0))\ .withColumn('event_month', f.format_string(""%04d-M%02d"", f.year(col('last_datetime_visit_h3_7')), f.month(col('last_datetime_visit_h3_7'))))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"f.format_string(""%04d-M%02d"", f.year(col('last_datetime_visit_h3_7')), f.month(col('last_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.month(col('last_datetime_visit_h3_7')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.year(col('last_datetime_visit_h3_7')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,2,,Function,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,col('last_datetime_visit_h3_7'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.format_string,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,233,notebook.Sample_3a. Pipeline Periodically - Tourism,False,False,NotSupported,"f.format_string(""%04d-M%02d"", f.year(col('last_datetime_visit_h3_7')), f.month(col('last_datetime_visit_h3_7')))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.month,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.month(col('last_datetime_visit_h3_7')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.year,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,233,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,f.year(col('last_datetime_visit_h3_7')),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,235,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_6_tourism.withColumn('suffix', F.substring('msisdn', - 1, 1))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,235,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,235,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('msisdn', - 1, 1)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.withColumn,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,236,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"mobility_6_tourism.withColumn('execute_month', F.substring(F.lit(execute_month), 1, 7))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,236,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit(execute_month),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,236,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring(F.lit(execute_month), 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,236,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit(execute_month),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,236,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring(F.lit(execute_month), 1, 7)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,237,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,mobility_6_tourism.filter(F.col('event_month') == execute_month[:4] + '-M' + execute_month[- 2:]),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,237,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,237,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('event_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,12,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,14,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,14,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.createDataFrame,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.createDataFrame(jak_zone),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,14,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pandas.io.parsers.readers.TextFileReader"",""ParamType"":""TextFileReader""}]",Python
pyspark.sql.dataframe.DataFrame.repartition,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"tourism.repartition(100).write.partitionBy('execute_month', 'event_month', 'suffix').mode('append').saveAsTable('pnt_bps_int_stg.data_cerdas_tourism_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.write,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.repartition(100).write.partitionBy('execute_month', 'event_month', 'suffix').mode('append').saveAsTable('pnt_bps_int_stg.data_cerdas_tourism_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameWriter.mode,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.repartition(100).write.partitionBy('execute_month', 'event_month', 'suffix').mode('append').saveAsTable('pnt_bps_int_stg.data_cerdas_tourism_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.partitionBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Rename,"tourism.repartition(100).write.partitionBy('execute_month', 'event_month', 'suffix').mode('append').saveAsTable('pnt_bps_int_stg.data_cerdas_tourism_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameWriter.saveAsTable,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Transformation,"tourism.repartition(100).write.partitionBy('execute_month', 'event_month', 'suffix').mode('append').saveAsTable('pnt_bps_int_stg.data_cerdas_tourism_sample')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,18,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.sql(q).toPandas().tail(11),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.session.SparkSession.sql,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.sql(q).toPandas().tail(11),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,20,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_tourism_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_tourism_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.filter(F.col('execute_month') == F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit('-'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 1, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.concat,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit('-'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 1, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,21,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,6,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.countDistinct('msisdn', 'move_trip', 'move_visit').alias('num_visit'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,22,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.readwriter.DataFrameReader.table,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.session.SparkSession.read,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,spark.read.table('pnt_bps_int_stg.data_cerdas_mobility_trip_sample'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.dataframe.DataFrame.filter,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.filter(F.col('execute_month') == F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2)))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""bool"",""ParamType"":""bool""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit('-'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 1, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.col,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.col('execute_month'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.concat,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.concat(F.substring('event_month', 1, 4), F.lit('-'), F.substring('event_month', 7, 2))",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.functions.lit,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.lit('-'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 1, 4)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.functions.substring,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.substring('event_month', 7, 2)",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,24,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""},{""ParamName"":""int"",""ParamType"":""int""},{""ParamName"":""int"",""ParamType"":""int""}]",Python
pyspark.sql.dataframe.DataFrame.groupBy,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.group.GroupedData.agg,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,1,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""},{""ParamName"":""pyspark.sql.column.Column"",""ParamType"":""Column""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,2,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.countDistinct('msisdn').alias('num_msisdn'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.countDistinct,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,3,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"F.countDistinct('msisdn', 'move_trip').alias('num_trip')",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""},{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.column.Column.alias,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""True""}]",Python
pyspark.sql.functions,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Module,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
pyspark.sql.functions.count,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,4,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,F.count('msisdn').alias('num_row'),0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,"[{""ParamName"":""str"",""ParamType"":""str"",""IsRegex"":""False""}]",Python
pyspark.sql.dataframe.DataFrame.toPandas,notebook,Sample_3a. Pipeline Periodically - Tourism.ipynb,1,,Function,5,notebook.Sample_3a. Pipeline Periodically - Tourism,True,True,Direct,"tourism.groupBy('event_month').agg(F.countDistinct('msisdn').alias('num_msisdn'), F.countDistinct('msisdn', 'move_trip').alias('num_trip'), F.count('msisdn').alias('num_row'), ).toPandas()",0f552bec-ad97-4d62-8e6b-5f58f82cb32c,8.00.019,1.27.000,25,a1cbc006-17c0-4db1-a282-bccf0e51f12f,,Python
