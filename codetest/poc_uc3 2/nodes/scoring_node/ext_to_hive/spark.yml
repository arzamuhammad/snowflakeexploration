spark.submit.deployMode :  "client"
spark.executor.memory :  "32g"
spark.executor.instances :  "4"
spark.executor.cores :   "5"
spark.sql.shuffle.partitions : "500"
spark.driver.memory :   "15g"
spark.shuffle.service.enabled : "true"
spark.shuffle.file.buffer : "1MB"
spark.dynamicAllocation.enabled : "true"
spark.dynamicAllocation.minExecutors : "5"
spark.dynamicAllocation.maxExecutors : "20"
spark.dynamicAllocation.initialExecutors : "15"
spark.yarn.maxAppAttempts : "2"
spark.sql.parquet.compression.codec : "snappy"
spark.driver.memoryOverhead : "8192"
spark.yarn.driver.memoryOverhead : "8192"
spark.yarn.executor.memoryOverhead : "8192"
spark.executor.heartbeatInterval : "20s"
spark.network.timeout : "50000s"
spark.sql.broadcastTimeout : "7200"
spark.ui.view.acls : "*"
spark.sql.hive.convertMetastoreParquet : "false"
hive.exec.dynamic.partition.mode : "nonstrict"
spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation : "true"
spark.driver.maxResultSize : '8g'
# spark.executor.maxResultSize : '16g'
spark.sql.hive.verifyPartitionPath :  "false"
spark.sql.files.ignoreMissingFiles :  "true"
spark.sql.files.ignoreCorruptFiles :  "true"
hive.input.dir.recursive :  "true"
hive.mapred.supports.subdirectories :  "true"
hive.supports.subdirectories :  "true"
mapred.input.dir.recursive :  "true"
spark.ui.enabled :  "false"
spark.sql.sources.partitionOverwriteMode : "dynamic"
spark.sql.codegen.wholeStage : "false"
spark.hadoop.hive.exec.max.dynamic.partitions : "10000"
spark.sql.sources.outputCommitterClass : "org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter"
